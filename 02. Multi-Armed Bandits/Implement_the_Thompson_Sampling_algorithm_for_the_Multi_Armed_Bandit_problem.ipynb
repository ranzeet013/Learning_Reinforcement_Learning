{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "number_of_arms = 10\n",
        "number_of_rounds = 1000\n",
        "\n",
        "true_probabilities = np.random.rand(number_of_arms)\n",
        "print(f\"True Probabilities : {true_probabilities}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALyk7YqBmIvB",
        "outputId": "01ce2348-0aff-40db-d498-e4c5f0c82f35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Probabilities : [0.73139792 0.18824286 0.199132   0.84180138 0.01826543 0.96539899\n",
            " 0.05505147 0.85034135 0.47424441 0.90637669]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thompson Sampling variables\n",
        "successes = np.zeros(number_of_arms)\n",
        "failures = np.zeros(number_of_arms)\n",
        "total_thompson_reward = 0\n",
        "thompson_rewards = np.zeros(number_of_rounds)\n",
        "cumulative_thompson_regret = np.zeros(number_of_rounds)\n",
        "\n",
        "# Optimal strategy variables\n",
        "best_arm = np.argmax(true_probabilities)\n",
        "optimal_reward = 0\n",
        "optimal_rewards = np.zeros(number_of_rounds)"
      ],
      "metadata": {
        "id": "Da3kKTZ-qFEf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thompson Sampling Algorithm\n",
        "for t in range(number_of_rounds):\n",
        "    sampled_values = [np.random.beta(successes[i] + 1, failures[i] + 1) for i in range(number_of_arms)]\n",
        "    arm_thompson = np.argmax(sampled_values)\n",
        "\n",
        "    reward_thompson = np.random.rand() < true_probabilities[arm_thompson]\n",
        "\n",
        "    if reward_thompson:\n",
        "        successes[arm_thompson] += 1\n",
        "    else:\n",
        "        failures[arm_thompson] += 1\n",
        "\n",
        "    total_thompson_reward += reward_thompson\n",
        "    thompson_rewards[t] = total_thompson_reward\n",
        "\n",
        "    optimal_rewards[t] = true_probabilities[best_arm]\n",
        "    optimal_reward += optimal_rewards[t]\n",
        "    cumulative_thompson_regret[t] = optimal_reward - total_thompson_reward\n",
        "\n",
        "print(f\"Total Reward: {total_thompson_reward}\")\n",
        "print(f\"Optimal Reward: {optimal_reward}\")\n",
        "print(f\"Cumulative Regret: {cumulative_thompson_regret[-1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xnuOLcVqsZV",
        "outputId": "b3f20c41-8f26-4169-827f-e8f6d77a01ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Reward: 932\n",
            "Optimal Reward: 965.3989873468162\n",
            "Cumulative Regret: 33.39898734681617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgU7bhozsfoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}