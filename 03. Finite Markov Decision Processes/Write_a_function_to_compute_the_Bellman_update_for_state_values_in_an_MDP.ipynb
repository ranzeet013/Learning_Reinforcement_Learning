{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Bellman Update Function**"
      ],
      "metadata": {
        "id": "yXduTCH3ufvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HPNVoiFVtv39"
      },
      "outputs": [],
      "source": [
        "def bellman_update_basic(V, state, transitions, rewards, discount_factor):\n",
        "    \"\"\"\n",
        "    Compute the Bellman update for a given state in an MDP.\n",
        "\n",
        "    Parameters:\n",
        "    - V: A dictionary representing the value function. Keys are states, and values are the value estimates.\n",
        "    - state: The state for which the Bellman update is computed.\n",
        "    - transitions: A dictionary representing the transition probabilities. Keys are (state, action) pairs,\n",
        "      and values are lists of (next_state, probability) pairs.\n",
        "    - rewards: A dictionary representing the rewards. Keys are (state, action) pairs, and values are the rewards.\n",
        "    - discount_factor: The discount factor (gamma), a float in [0, 1).\n",
        "\n",
        "    Returns:\n",
        "    - The updated value for the given state.\n",
        "    \"\"\"\n",
        "    max_value = float('-inf')\n",
        "    for action in transitions[state]:\n",
        "        expected_value = 0\n",
        "        for next_state, prob in transitions[state][action]:\n",
        "            expected_value += prob * (rewards[(state, action)] + discount_factor * V[next_state])\n",
        "        max_value = max(max_value, expected_value)\n",
        "\n",
        "    return max_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bellman Update with Detailed Decomposition**"
      ],
      "metadata": {
        "id": "jQLhCGDMun0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bellman_update_decomposed(V, state, transitions, rewards, discount_factor):\n",
        "    \"\"\"\n",
        "    Compute the Bellman update for a given state in an MDP with more decomposition for clarity.\n",
        "\n",
        "    Parameters:\n",
        "    - V: A dictionary representing the value function.\n",
        "    - state: The state for which the Bellman update is computed.\n",
        "    - transitions: A dictionary of transition probabilities.\n",
        "    - rewards: A dictionary of rewards.\n",
        "    - discount_factor: The discount factor (gamma).\n",
        "\n",
        "    Returns:\n",
        "    - Updated value for the given state.\n",
        "    \"\"\"\n",
        "    action_values = []\n",
        "\n",
        "    for action in transitions[state]:\n",
        "        expected_value = compute_expected_value(V, state, action, transitions, rewards, discount_factor)\n",
        "        action_values.append(expected_value)\n",
        "\n",
        "    return max(action_values)\n",
        "\n",
        "\n",
        "def compute_expected_value(V, state, action, transitions, rewards, discount_factor):\n",
        "    \"\"\"\n",
        "    Compute the expected value for a given state-action pair.\n",
        "\n",
        "    Parameters:\n",
        "    - V: The value function.\n",
        "    - state: The current state.\n",
        "    - action: The action taken.\n",
        "    - transitions: Transition probabilities.\n",
        "    - rewards: Rewards associated with state-action pairs.\n",
        "    - discount_factor: The discount factor (gamma).\n",
        "\n",
        "    Returns:\n",
        "    - The expected value for the state-action pair.\n",
        "    \"\"\"\n",
        "    expected_value = 0\n",
        "    for next_state, prob in transitions[state][action]:\n",
        "        expected_value += prob * (rewards[(state, action)] + discount_factor * V[next_state])\n",
        "\n",
        "    return expected_value\n"
      ],
      "metadata": {
        "id": "JD9jJT-iujjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bellman Update with Numpy**"
      ],
      "metadata": {
        "id": "-Z-uCRhMu1TS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def bellman_update_numpy(V, state, transitions, rewards, discount_factor):\n",
        "    \"\"\"\n",
        "    Compute the Bellman update for a given state in an MDP using numpy for better performance.\n",
        "\n",
        "    Parameters:\n",
        "    - V: A numpy array representing the value function.\n",
        "    - state: The index of the state for which the Bellman update is computed.\n",
        "    - transitions: A list of lists representing the transition probabilities.\n",
        "    - rewards: A numpy array of rewards.\n",
        "    - discount_factor: The discount factor (gamma).\n",
        "\n",
        "    Returns:\n",
        "    - The updated value for the given state.\n",
        "    \"\"\"\n",
        "    action_values = np.zeros(len(transitions[state]))\n",
        "\n",
        "    for action in range(len(transitions[state])):\n",
        "        next_states, probs = zip(*transitions[state][action])\n",
        "        rewards_for_action = rewards[state, action]\n",
        "        future_rewards = discount_factor * V[next_states]\n",
        "        action_values[action] = np.dot(probs, rewards_for_action + future_rewards)\n",
        "\n",
        "    return np.max(action_values)\n"
      ],
      "metadata": {
        "id": "23F_0GTGurGp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}