{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rBWlfF3YQg21"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def asynchronous_value_iteration(states, actions, transition_model, reward_function, discount_factor=0.9, theta=1e-6):\n",
        "    \"\"\"\n",
        "    Performs asynchronous value iteration to solve a Markov Decision Process (MDP) with a large state space.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    states : list or array-like\n",
        "        A list of all possible states in the MDP.\n",
        "\n",
        "    actions : list or array-like\n",
        "        A list of all possible actions in the MDP.\n",
        "\n",
        "    transition_model : function\n",
        "        A function `transition_model(s, a)` that returns a list of tuples `(s_next, prob)` where:\n",
        "        - `s_next` is a possible next state given state `s` and action `a`.\n",
        "        - `prob` is the probability of transitioning to state `s_next`.\n",
        "\n",
        "    reward_function : function\n",
        "        A function `reward_function(s, a, s_next)` that returns the reward obtained by taking action `a` in state `s`\n",
        "        and transitioning to state `s_next`.\n",
        "\n",
        "    discount_factor : float, optional (default=0.9)\n",
        "        The discount factor `gamma` used in the Bellman equation. It should be a value between 0 and 1.\n",
        "\n",
        "    theta : float, optional (default=1e-6)\n",
        "        A small threshold for determining when to stop the iteration. The iteration stops when the maximum change\n",
        "        in the value function across all states is less than `theta`.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    V : numpy.ndarray\n",
        "        An array representing the value function `V(s)` for each state `s` in the state space.\n",
        "\n",
        "    Notes:\n",
        "    ------\n",
        "    - This function performs asynchronous updates, meaning that each state is updated in sequence rather than simultaneously.\n",
        "    - The function assumes a finite and discrete state and action space.\n",
        "    - This implementation is memory-efficient and can be adapted to large state spaces by leveraging techniques such as\n",
        "      sparse representations, prioritized sweeping, and state aggregation.\n",
        "    - The algorithm stops iterating when the value function converges, i.e., when the maximum difference in value across\n",
        "      all states between two successive iterations is less than `theta`.\n",
        "\n",
        "    Example Usage:\n",
        "    --------------\n",
        "    states = [0, 1, 2]  # Example states\n",
        "    actions = ['a', 'b']  # Example actions\n",
        "\n",
        "    def transition_model(s, a):\n",
        "        # Example transition model returning next states and probabilities\n",
        "        if s == 0:\n",
        "            return [(1, 0.8), (2, 0.2)]\n",
        "        elif s == 1:\n",
        "            return [(0, 0.6), (2, 0.4)]\n",
        "        else:\n",
        "            return [(0, 1.0)]\n",
        "\n",
        "    def reward_function(s, a, s_next):\n",
        "        # Example reward function\n",
        "        if s == 0 and a == 'a' and s_next == 1:\n",
        "            return 5\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    V = asynchronous_value_iteration(states, actions, transition_model, reward_function)\n",
        "    print(V)\n",
        "    \"\"\"\n",
        "    V = np.zeros(len(states))  # Initialize value function\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in states:\n",
        "            v = V[s]\n",
        "            max_value = float('-inf')\n",
        "            for a in actions:\n",
        "                expected_value = sum([prob * (reward_function(s, a, s_next) + discount_factor * V[s_next])\n",
        "                                      for s_next, prob in transition_model(s, a)])\n",
        "                max_value = max(max_value, expected_value)\n",
        "            V[s] = max_value\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GZW_WuNjRc0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}